# GPT-2 Experimentation

## Overview
In this exercise, I experimented with transformer models, specifically with GPT-2, a generative model from the GPT (Generative Pretrained Transformers) family. GPT-2 was publicly released by OpenAI in 2019 and is available through Huggingface. You can learn more about GPT-2 at the following link: [GPT-2 Wikipedia](https://en.wikipedia.org/wiki/GPT-2), while documentation for the Huggingface model can be found here: [Huggingface GPT-2 Documentation](https://huggingface.co/docs/transformers/model_doc/gpt2).

Within the scope of this task, I used the smaller model, with 124 million parameters (the larger one has 1.5 billion). Initially, I utilized the pretrained model, and then I fine-tuned it on a dataset consisting of starwars subtitles. Also there is a bonus question, which asks students to find the mistake in the code of the Multihead Attention class provided in this [implementation](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch).

## Deliverables & Notes
Deliverables will include the Python code and a report. The code may consist of multiple files, and Python notebooks are also acceptable.

## Results Report
I have prepared a comprehensive report documenting the results of the experiments. The report includes visualizations such as plots and charts for better understanding and interpretation of the outcomes. **The report was first written in Greek, then translated automatically into English (using this [tool](https://www.onlinedoctranslator.com/en/)).**
