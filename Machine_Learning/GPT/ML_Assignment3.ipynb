{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtnO_3cDW4Vp"
      },
      "source": [
        "# Part 1: Using GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcdtrG2yW4V3"
      },
      "source": [
        "## Testing with top-k (answer to 1st & a part of 2nd question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP3MKXqjW4V4",
        "outputId": "115146b3-f6b6-4468-ed6d-8c54ac8fd82e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt > Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife.\n",
            "Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife. After he had made some changes, the Indians began to grow and multiply. The great land was given to us as \"the country that gave birth to the man's body,\" and the Indians had to be removed from the lands to be placed in other lands. At last the Indians were able to take possession of the land and the land was taken, and we, as a whole, have a right to a presentation of this same soil.\n",
            "\n",
            "We do not want to have our presentations to be a part of it, because, as we know, we have been in that territory many and many years, and the Indians have a lot more land, and they must give us a presentation before we can do that again. We have had no right, in any part of this country ever to have a part whatever in our presentations. No right to make this land our presentation; we have always had a right to the presentation, for it is what we have been through\n",
            "=======================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt > To be or not to be: that is the question.\n",
            "To be or not to be: that is the question.\n",
            "\n",
            "But to me, this was not about this. This was about the fact — it was something that was part of that whole debate. And now here's the thing, if you're a feminist or a humanist and you've been doing this long enough, you'll realize that, I think, the question that you have to ask when you say \"feminism\" or \"humanism\" is whether or not that's what you really think of as feminism — and to me, feminism as a concept, as a concept as it relates to things that, I think, are more or less the same concept. And I have to say, this is one of those things that's a pretty simple and easy answer and that's the sort of question that makes me think in that way. But at the same time, that's just something that I've been doing and that's the sort of thing that I think is going to make feminists, and I will say this, if it's\n",
            "=======================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt > In a galaxy far, far away,\n",
            "In a galaxy far, far away, in a galaxy far, away, in a galaxy far, the world is a place where you can go to the right place in the right time and place of the wrong time. And in all of that time, if I'm on the wrong side, what can I do to change it?\n",
            "\n",
            "That's where we're going. You're going from a time where you're sitting in a room with a television and you're talking and you're thinking about things and you're looking for the right thing that has to do with the season and the people you want to find. The season is already done. We're at a point where we're able to get things done. So there's no question about that.\n",
            "\n",
            "But let's look at this season. What kind of season is there in the universe where there is this idea that there's an entire universe that's in a perfect place in which you can have a family without having your spouse be on the wrong side of something?\n",
            "=======================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "for i in range(3):\n",
        "    text = input(\"Prompt: > \")\n",
        "    print(\"Prompt >\", text)\n",
        "    encoded_text = tokenizer(text, return_tensors='pt')\n",
        "    encoded_text = encoded_text.to(device)\n",
        "    response = model.generate(**encoded_text, max_new_tokens=200, do_sample=True, top_k=20)\n",
        "    response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "    print(response_text)\n",
        "    print(\"=======================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cBWStIFW4V7"
      },
      "source": [
        "## Creating the model for easier testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSlGQhF_W4V7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VP155twW4V8"
      },
      "source": [
        "## Prompts used for testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA8uv9akW4V8"
      },
      "outputs": [],
      "source": [
        "prompts = [\"Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife.\", \"In a galaxy far, far away,\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGGScSBeW4V9"
      },
      "source": [
        "## Testing greedy approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f90z-HLW4V9",
        "outputId": "4aa3abeb-13dc-4871-96ce-9ce1466e45ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt > Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife. The two of them were very close, and they were very good friends.\n",
            "\n",
            "The next day, the two of them went to the house of the farmer, and there they met the two of them, and they were very happy. They were very happy, and they were very happy.\n",
            "\n",
            "The next day, the two of them went to the house of the farmer, and there they met the two of them, and they were very happy. They were very happy, and they were very happy.\n",
            "\n",
            "The next day, the two of them went to the house of the farmer, and there they met the two of them, and they were very happy. They were very happy, and they were very happy.\n",
            "\n",
            "The next day, the two of them went to the house of the farmer, and there they met the two of them, and they were very happy. They were very happy, and they were very happy.\n",
            "\n",
            "The next day, the two of\n",
            "=======================================\n",
            "Prompt > In a galaxy far, far away,\n",
            "In a galaxy far, far away, the galaxy is a vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast,\n",
            "=======================================\n"
          ]
        }
      ],
      "source": [
        "for text in prompts:\n",
        "    print(\"Prompt >\", text)\n",
        "    encoded_text = tokenizer(text, return_tensors='pt')\n",
        "    encoded_text = encoded_text.to(device)\n",
        "    response = model.generate(**encoded_text, max_new_tokens=200, do_sample=False)\n",
        "    response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "    print(response_text)\n",
        "    print(\"=======================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWq_-oVVW4V-"
      },
      "source": [
        "## Testing with Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26hLL4DWW4V-",
        "outputId": "83fdc75b-10fc-4f0f-9f30-6d24e5bc11f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt > Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife.\n",
            "\n",
            "In the spring of 1848, the family moved to a small town on the western side of the Missouri River, near the Missouri River.\n",
            "\n",
            "In the spring of 1848, the family moved to a small town on the western side of the Missouri River, near the Missouri River.\n",
            "\n",
            "In the spring of 1848, the family moved to a small town on the western side of the Missouri River, near the Missouri River.\n",
            "\n",
            "In the spring of 1848, the family moved to a small town on the western side of the Missouri River, near the Missouri River.\n",
            "\n",
            "In the spring of 1848, the family moved to a small town on the western side of the Missouri River, near the Missouri River.\n",
            "\n",
            "In the spring of 1848, the family moved to a small town on the western side of the Missouri River, near the Missouri River.\n",
            "\n",
            "In the spring of 1848, the family moved to a small town on the western side\n",
            "=======================================\n",
            "Prompt > In a galaxy far, far away,\n",
            "In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "\n",
            "In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "\n",
            "In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "\n",
            "In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "\n",
            "In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "\n",
            "In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "\n",
            "In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "\n",
            "In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "\n",
            "In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "\n",
            "In a galaxy far, far away, there is a galaxy far, far, far away.\n",
            "=======================================\n"
          ]
        }
      ],
      "source": [
        "for text in prompts:\n",
        "    print(\"Prompt >\", text)\n",
        "    encoded_text = tokenizer(text, return_tensors='pt')\n",
        "    encoded_text = encoded_text.to(device)\n",
        "    response = model.generate(**encoded_text, max_new_tokens=200, num_beams=5, early_stopping=True)\n",
        "    response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "    print(response_text)\n",
        "    print(\"=======================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGQQIL6eW4V_"
      },
      "source": [
        "### Using n-grams with beam search -> better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vbjZ9mcW4V_",
        "outputId": "8387842a-fcd6-401b-cf41-e1a2add2c5ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt > Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife.\n",
            "\n",
            "In the spring of 1848, the family moved to a small town on the outskirts of Kansas City, Missouri, where they lived for a few years. In 1849, they moved back to their old home and moved into their new home, which is now the home of their great-great-grandmother, Mrs. Elizabeth Dorothy. They have lived there for many years, but have not been able to find a home for themselves, so they have been forced to move into a new house. The house was built in 1851 and has been in use since that time. It is a beautiful house with a large garden, a well-maintained yard, an open-air swimming pool and a fire pit. There are two bedrooms, one for the children and the other for their mother and father, as well as a living room, dining room and dining-room, all of which are in good condition. A large fireplace is located on one side of this house,\n",
            "=======================================\n",
            "Prompt > In a galaxy far, far away,\n",
            "In a galaxy far, far away, there is no such thing as a safe place to be.\n",
            "\n",
            "It's a place where you can be anything you want. You can go anywhere, anywhere. There's no place like home. It's the only place you'll ever be able to go, and that's why I'm here. I want you to know that I love you. And I know you're going to love me, too.\n",
            "=======================================\n"
          ]
        }
      ],
      "source": [
        "for text in prompts:\n",
        "    print(\"Prompt >\", text)\n",
        "    encoded_text = tokenizer(text, return_tensors='pt')\n",
        "    encoded_text = encoded_text.to(device)\n",
        "    response = model.generate(**encoded_text, max_new_tokens=200, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "    response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "    print(response_text)\n",
        "    print(\"=======================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alWJT8fyW4WA"
      },
      "source": [
        "## Testing with top-p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-WPH3xVW4WA",
        "outputId": "653e6aed-d417-42ce-8bd5-0f52bac92bc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt > Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife. They married in 1479, to whom they have continued to live ever since. The two children were: George a son of Samuel, who died of heart disease in 1477; and Alice a daughter of Samuel, who died of disease in 1484. In consequence, they live well, have a common life, have a good home, and live in peace.\n",
            "\n",
            "The eldest daughter, a daughter of Edward, died in 1492; the son is buried in the old Church of England in Leicester, which she lived in until the mid-1630s; his widow, Esther, died in 1650.\n",
            "\n",
            "In 1744, Charles Dandridge became a member of the Continental Parliament. The Continental Parliament was elected in 1745. It was formed with the support of a majority in Congress; the president was John Adams, who was appointed by George Washington, a member of the Continental Parliament in 1776, and John Jay, a member of Congress until 1801. When Jay\n",
            "=======================================\n",
            "Prompt > In a galaxy far, far away,\n",
            "In a galaxy far, far away, one of the brightest galaxies that ever exists, lies hidden beneath the sky.\n",
            "\n",
            "In a galaxy far, far away, one of the brightest galaxies that ever exists, lies hidden beneath the sky. The universe looks really, really dark. When NASA's Keck Observatory and the SETI program found the galaxy, it became a mystery and was left out of the search. The Keck observatory, which includes Kota and Chico, in California, also turned up a giant, glowing gas giant called a dwarf galaxy known as M1. But a team of scientists from UCLA, New Mexico and California State University has found something even stranger: They can observe it from inside a star.\n",
            "\n",
            "The Keck Telescope, now under construction at the European Southern Observatory in Chile, is the largest telescope in the world to study the Keck Cluster. The telescope has a broad field of view of at least three million light-years, in contrast to the thousands or even millions that are visible over\n",
            "=======================================\n"
          ]
        }
      ],
      "source": [
        "for text in prompts:\n",
        "    print(\"Prompt >\", text)\n",
        "    encoded_text = tokenizer(text, return_tensors='pt')\n",
        "    encoded_text = encoded_text.to(device)\n",
        "    response = model.generate(**encoded_text, max_new_tokens=200, do_sample=True, top_p=0.95)\n",
        "    response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "    print(response_text)\n",
        "    print(\"=======================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edRz6fE7W4WB"
      },
      "source": [
        "## Testing with top-p and top-k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfpI7V-3W4WB",
        "outputId": "b3096d44-a508-43f1-af96-29ca5e9b15eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt > Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's wife. On the same day, she and her father took a boat to the town, where they were found by a man from the same company. The young woman and the young man who accompanied them were killed in the collision with a herd of cattle, and they were buried at the cemetery. They were buried in a large grave at Woburn, a little north of Wichita, and the remains of Mrs. Ortega are said to be there. They were buried in a large mound on a hillside near the town. There was an old brick church built in 1837. The church of Mrs. Ortega was found in the vicinity of the burial place of her mother. Another church in which the family is said to have lived is the Methodist Church, at Kuehlman, Kansas City. The oldest surviving church is located in the churchyard of the church where they lived. It is said that they were baptized at the stake and that they were buried near the spot where they died\n",
            "=======================================\n",
            "Prompt > In a galaxy far, far away,\n",
            "In a galaxy far, far away, and from a galaxy far, far away, is a strange, strange world.\n",
            "\n",
            "The galaxy is a vast, unspoiled, multiverse, filled with many galaxies, and many planets, many universes, many universes, many universes, and many universes. Every universe is inhabited by several intelligent beings, and even many of them have their own unique characteristics, but the main star in the universe is the largest galaxy in the entire known universe and is the most powerful. And this is not all.\n",
            "\n",
            "This planet is the \"Gigantic Moon\" and all the stars that inhabit it are of this world. Every star is one massive, massive star. Every star is made up of billions of galaxies. Each galaxy is filled with trillions of stars that are all the same size. Each galaxy is home to millions of galaxies. Every galaxy is a huge star in which there are many galaxies and billions of galaxies and millions of planets, billions of planets, and hundreds of planets. So\n",
            "=======================================\n"
          ]
        }
      ],
      "source": [
        "for text in prompts:\n",
        "    print(\"Prompt >\", text)\n",
        "    encoded_text = tokenizer(text, return_tensors='pt')\n",
        "    encoded_text = encoded_text.to(device)\n",
        "    response = model.generate(**encoded_text, max_new_tokens=200, do_sample=True, top_p=0.95, top_k=20)\n",
        "    response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "    print(response_text)\n",
        "    print(\"=======================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP6VPCqEW4WB"
      },
      "source": [
        "# Part 2: Model Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model.\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   100 of 2,753. Loss: 0.04650292173027992.Elapsed: 0:00:47.\n",
            "0: (very cool) Are you sure?\n",
            "It has been rather a long time. Do you think he'll be around long?\n",
            "No, I'd just be a little while longer.\n",
            "What's he's doing? He's going to pull us all apart.\n",
            "Go get him!\n",
            "We've got to find him. Open the back door, isn't it?\n",
            "I would much rather have gone with Master Luke. I'm sure it's safe for droids.\n",
            "What's your doing?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   200 of 2,753. Loss: 0.11145934462547302.Elapsed: 0:01:37.\n",
            "0: We ran into Count Dooku.\n",
            "I'm so worried about you.\n",
            "I'm not upset?\n",
            "You worry about something?\n",
            "I worry about you.\n",
            "You worry about something.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   300 of 2,753. Loss: 0.17811453342437744.Elapsed: 0:02:26.\n",
            "0: Then we will find out.\n",
            "I promise.\n",
            "Bongo du bongu!\n",
            "Goodie!\n",
            "Are you...\n",
            "Let's see.\n",
            "It's not bad.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   400 of 2,753. Loss: 0.0762481540441513.Elapsed: 0:03:15.\n",
            "0: I heard a rumor they are going to banish all droids.\n",
            "I have not seen one of these since I was prospecting on Subterrel beyond the Outer Rim!\n",
            "Do you know where it all lies?\n",
            "I would much rather avoid any conflict.\n",
            "Master Jedi, may I suggest that the Senator be placed under the protection of your graces.\n",
            "Do you know where it came from?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   500 of 2,753. Loss: 0.08602133393287659.Elapsed: 0:04:05.\n",
            "0: Not fit? Why would anyone think that?\n",
            "They say his mind has become fogged by the influence of a certain female Senator.\n",
            "That's ridiculous. Who?!?\n",
            "(slylylylylylylylylylylyly-looking) No! It's not me!\n",
            "(looking down) No! Why are you here?\n",
            "Oh, I'm here!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   600 of 2,753. Loss: 0.03201024606823921.Elapsed: 0:04:56.\n",
            "0: It couldn't happen here. You said it yourself. The Empire won't bother with this rock.\n",
            "Things always change.\n",
            "I wish I was going... Are you going to be around long?\n",
            "No, I'm leaving in the morning...\n",
            "Then I guess I won't see you.\n",
            "Maybe someday... I'll keep a lookout.\n",
            "Well, I'll be at the Academy next season... after that who knows.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   700 of 2,753. Loss: 0.05538511648774147.Elapsed: 0:05:47.\n",
            "0: (nods, gasping) Ben... Ben.\n",
            "Luke... Ben... Ben.\n",
            "Hang on, kid.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   800 of 2,753. Loss: 0.09699788689613342.Elapsed: 0:06:37.\n",
            "0: I don't think so.\n",
            "Well, I suppose I shouldn't expect much resistance.\n",
            "I'm not going to be a problem for you, my young apprentice.\n",
            "You have been a good apprentice. You are much wiser than Iam.\n",
            "All right, old Ben. How about you?\n",
            "I don't know.\n",
            "You're just not in time for dinner. I hope you have a great honor. I'm just a boy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   900 of 2,753. Loss: 0.05032370984554291.Elapsed: 0:07:27.\n",
            "0: Not this time. Something's not right because now I can't see. Wait. Wait! Oh, my! what have you done? I'm backwards, you stupid furball. Only an overgrown mophead like you would be stupid enough...\n",
            "I feel terrible.\n",
            "Why are they doing this?\n",
            "They never even asked me any questions.\n",
            "Lando.\n",
            "Get help!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,000 of 2,753. Loss: 0.06580036878585815.Elapsed: 0:08:18.\n",
            "0: Not this time. There's too much at stake. We need help. Odd Ball, do you copy?\n",
            "\"This time I won't let these visions come true\n",
            "I won't let these visions come true\n",
            "Death is a natural part of life\n",
            " Rejoice for those around you who transform into the Force. Mourn them, do not. Miss them, do not. Attachment leads to jealousy and pride. The shadow of greed, that is.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,100 of 2,753. Loss: 0.03251463547348976.Elapsed: 0:09:09.\n",
            "0: Hush! Not so loud! (arriving) With the blast shields down, I can't even see.\n",
            "Move! Come on, Artoo! Quickly, Artoo.\n",
            "Get in gear, Artoo. There's a lot of this.\n",
            "(into comlink) Artoo, where are you? We're going to attack the ship right before you can.\n",
            "(into comlink) Artoo, where are you? I'm going to cut across the axis and try and draw their fire.\n",
            "(into comlink) Artoo, where are you? Oh, this is no time for heroics. Come on, Artoo. Oh!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,200 of 2,753. Loss: 0.048692647367715836.Elapsed: 0:10:01.\n",
            "0: (very little) No. They are doing their job so we can do ours. Head for the Command Ship!\n",
            "Missiles! Pull up!\n",
            "They overshot us...\n",
            "They've coming around!\n",
            "All right, Arfour. No, no, no. Nothing too fancy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,300 of 2,753. Loss: 0.0441356785595417.Elapsed: 0:10:51.\n",
            "0: No! Forget not, this has been a day long remembered. It has seen the end of Kenobi and it will soon see the end of the Rebellion.\n",
            "(over loudspeaker) All flight trooper, man your stations.\n",
            "So... you got your reward and you're just leaving then?\n",
            "That's right, yeah! I got some old debts I've got to pay off with this stuff. Even if I didn't, you don't think I'd be fool enough to stick around here, do you? Why don't you come with us? You're pretty good in a fight. I could use you.\n",
            "You could use you.\n",
            "(getting angry) Come on! Why don't you take a look around? You know what's about to happen, what they're up against. They could use a little more time.\n",
            "What is it?\n",
            "Nothing.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,400 of 2,753. Loss: 0.07412653416395187.Elapsed: 0:11:43.\n",
            "0: You turned her against me fair and square.\n",
            "You have done well, Anakin. She is far too trusting.\n",
            "I have advised you over the years, my little friend. Do you feel your power growing?\n",
            "Yes, My Master.\n",
            "Now, Lord Sidious. Do you have any idea who was behind it?\n",
            "Our intelligence points to disgruntled spice miners, on the moons of Naboo. We also know that relations between the Council and the Chancellor are stressed.\n",
            "I don't trust the Council... I can assure you they will do what it takes to share in it.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,500 of 2,753. Loss: 0.054016754031181335.Elapsed: 0:12:34.\n",
            "0: No, Annie. You're safe. What?\n",
            "I'm fine. You had a nightmare.\n",
            "But you don't look so bad to me. In fact, you look strong enough to pull the ears off a Gundark.\n",
            "Thanks to you.\n",
            "That's two you owe me, junior.\n",
            "Well your Worship, looks like you managed to keep me around for a little while longer.\n",
            "(haughtily) I had nothing to do with it. General Rieekan thinks it's dangerous for any ships to leave the system until we've activated the energy shield.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,600 of 2,753. Loss: 0.025049585849046707.Elapsed: 0:13:26.\n",
            "0: You're gravely mistaken. You won't convert me as you did my father.\n",
            "Oh, no, my young Jedi. You will find that it is you who are mistaken...about a great many things.\n",
            "His lightsaber.\n",
            "Ah, yes, a Jedi's weapon. Much like your father's. By now you must know your father can never be turned from the dark side. So will it be with you.\n",
            "You're wrong. Soon I'll be dead...and you with me.\n",
            "Perhaps you refer to the imminent attack of your Rebel fleet.\n",
            "Yes...I assure you we are quite safe from your friends here.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,700 of 2,753. Loss: 0.034302882850170135.Elapsed: 0:14:17.\n",
            "0: This scheme of yours has failed, Lord Sidious. The blockade is finished. We dare not go against these Jedi.\n",
            "Viceroy, is the planet secure?\n",
            "Yes, my Lord, we have taken over the last pockets of primitive life forms. We have been without an interpreter since our master got angry with our last protocol droid and disintegrated him.\n",
            "That's impossible, there's no one to talk to us, only Master.\n",
            "How would I explain this?\n",
            "You will learn from Yoda, the Jedi Master who instructed me. Your thoughts will become a Jedi must be grounded around you.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,800 of 2,753. Loss: 0.03992130234837532.Elapsed: 0:15:09.\n",
            "0: I can't believe that.\n",
            "I couldn't!\n",
            "The ship is almost finished. Two or Three more things and we're in great shape.\n",
            "The sooner the better. Something's wrong here. No one has seen or knows anything about Threepio. He's been gone too long to have gotten lost.\n",
            "Relax. I'll talk to Lando and see what I can find out.\n",
            "I don't trust him, either. But he is my friend. Besides, we'll soon be gone.\n",
            "And then you're as good as gone, aren't you?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,900 of 2,753. Loss: 0.04904940351843834.Elapsed: 0:16:00.\n",
            "0: Oh, Anakin, I'm afraid.\n",
            "Have faith, my love. Everything here is magical.\n",
            "You could look into the glass and see the water. The way it ripples and moves. It looked so real... but it wasn’t.\n",
            "Sometimes, when you believe something to be real, it becomes real. Real enough, anyway...\n",
            "I used to think if you looked too deeply into glass, you would lose yourself.\n",
            "I think it's true...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,000 of 2,753. Loss: 0.03614240139722824.Elapsed: 0:16:51.\n",
            "0: (laughing) I knew it! They were sent to force a settlement, eh.\n",
            "Distract them. I will contact Lord Sidious.\n",
            "Are you brain-dead? I'm not going in there with two Jedi.Send a droid in. I want every part of this ship checked!\n",
            "Yes, sir.\n",
            "Is there anything I might do to help?\n",
            "Well, not unless you can alter time, speed up the harvest, or teleport me off this rock!\n",
            "I don't think I can help. I've got to go back.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,100 of 2,753. Loss: 0.032931532710790634.Elapsed: 0:17:42.\n",
            "0: It's too late!\n",
            "(a whisper) Luke, help me take this mask off.\n",
            "But you'll die.\n",
            "Nothing can stop that now. Just for once... let me look on you with my own eyes.\n",
            "(very weak) Now...go, my son. Leave me.\n",
            "No. You're coming with me. I can't leave you here. I've got to save you.\n",
            "You already have, Luke. You were right about me. Tell your sister...you were right.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,200 of 2,753. Loss: 0.029774853959679604.Elapsed: 0:18:33.\n",
            "0: Master Qui-Gon? Is Anakin all right?\n",
            "Anakin! Anakin! There he is. He's still alive. Get a medical capsule, immediately.\n",
            "Yes sir. Right away.\n",
            "Failed to stop the Sith Lord, I have. Still much to learn, there is...\n",
            "(V.O.) Patience. You will have time. I did not. When I became one with the Force I made a great discovery. With my training, you will be able to merge with the Force at will. Your physical self will fade away, but you will still retain your consciousness. You will become more powerful than any Sith.\n",
            "Eternal consciousness.\n",
            "(V.O.) You will become more powerful than any Sith.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,300 of 2,753. Loss: 0.028805602341890335.Elapsed: 0:19:25.\n",
            "0: (in Huttese subtitled) I told you not to admit him.\n",
            "I must be allowed to speak.\n",
            "(in Huttese subtitled) You must be allowed to speak.\n",
            "(in Huttese subtitled) You hear me, baby?\n",
            "You will not let me down.\n",
            "(continuing) I'm not allowed to speak.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,400 of 2,753. Loss: 0.024500321596860886.Elapsed: 0:20:16.\n",
            "0: Master.\n",
            "...Luke Skywalker, Jedi Knight.\n",
            "(in Huttese subtitled) I told you not to admit him.\n",
            "I must go, Master.\n",
            "Then you must go to the Sanctuary Moon and wait for him.\n",
            "(in Huttese subtitled) He must come in?\n",
            "We have powerful friends. You're gonna regret this...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,500 of 2,753. Loss: 0.02461281418800354.Elapsed: 0:21:06.\n",
            "0: You may be on the Council, but... they refused to accept me as a Jedi Master.\n",
            "Patience. In time, they will recognize your skills.\n",
            "They still treat me as if I were a Padawan learner... they fear my power, that's the problem.\n",
            "Anakin...\n",
            "Sometimes, when you believe something to be real, it becomes real. Real enough, anyway...\n",
            "I used to bull's- eye womp rats in my T-sixteen back home. They're not much bigger than two meters.\n",
            "Man your ships! And may the Force be with you!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,600 of 2,753. Loss: 0.029153993353247643.Elapsed: 0:21:58.\n",
            "0: Hey, ol' buddy!\n",
            "Hey, Dex.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,700 of 2,753. Loss: 0.029739856719970703.Elapsed: 0:22:48.\n",
            "0: The walls are moving! Don't just stand there. Try to brace it with something.\n",
            "Wait a minute!\n",
            "Threepio! Come in, Threepio! Threepio! Threepio!\n",
            "Get to the top!\n",
            "I can't\n",
            "Where could he be? Threepio! Threepio!\n",
            "Get him off of there!\n",
            "\n",
            " Average training loss: 0.06\n",
            " Training epoch took: 0:23:15\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   100 of 2,753. Loss: 0.015602454543113708.Elapsed: 0:00:50.\n",
            "0: I don't think they'll melt us down.\n",
            "Don't shoot! Don't shoot! Will this never end?\n",
            "Luke, tell Owen that if he gets a translator to be sure it speaks Bocce.\n",
            "It looks like we don't have much of a choice, but I'll remind him.\n",
            "I have no need for a protocol droid.\n",
            "(quickly) Sir -- not in an environment such as this -- that's why I've also been programmed for over thirty secondary functions that...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   200 of 2,753. Loss: 0.026772694662213326.Elapsed: 0:01:41.\n",
            "0: The attempt on my life has left me scarred and deformed, but I assure you my resolve has never been stronger.\n",
            "The war is over. (applause) The Separatists have been defeated, (applause) and the Jedi rebellion has been foiled. We stand on the threshold of a new beginning.\n",
            "Well, this is the moment we discover if he intends to return the Republic to a democracy.\n",
            "In order to ensure our security and continuing stability, the Republic will be reorganized into the first Galactic Empire, for a safe and secure society which I assure you will last for ten thousand years.\n",
            "(continuing) An empire that will continue to be ruled by this august body, and a sovereign ruler chosen for life...\n",
            "(continuing) An empire ruled by the majority... Ruled by a new constitution...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   300 of 2,753. Loss: 0.019943032413721085.Elapsed: 0:02:33.\n",
            "0: Why not? They seem to have a box of old coverings here.\n",
            "Oh? How observant of you, Miss Padme. Of course, I'm just not mechanically minded... if you see what I mean.\n",
            "Let's see, if we put this... here...\n",
            "Ooooh! That's tickles.\n",
            "You'll have to be quiet, THREEPIO. Hold still, please.\n",
            "Mom... Mom... Mom... Mom...\n",
            "Annie...? Is it you?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   400 of 2,753. Loss: 0.018832625821232796.Elapsed: 0:03:24.\n",
            "0: They're sealing this section off.\n",
            "Six droids coming our way!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   500 of 2,753. Loss: 0.03156033530831337.Elapsed: 0:04:14.\n",
            "0: I'm sorry, My Master.\n",
            "Remember, the war is a diversion. The Gungans will not easily be swayed, and we cannot use our power to help her.\n",
            "I'm I'm sorry, Viceroy Your trade boycott of our planet has ended.\n",
            "I was never aware of such a failure.\n",
            "I have word that the chancellor's ambassadors are with you now and that you have been commanded to reach settlement.\n",
            "I know nothing of any ambassadors. You must be mistaken.\n",
            "Beware, Viceroy the Federation is going too far this time.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   600 of 2,753. Loss: 0.02156873792409897.Elapsed: 0:05:05.\n",
            "0: What have we been up to?\n",
            "Our best troops have reached the swamp planet of Dantooine. They found the remains of a Rebel base, but they estimate that it has been deserted for some time. They are now conducting an extensive search of the surrounding systems.\n",
            "She lied! She lied to us!\n",
            "I told you she would never consciously betray the Rebellion.\n",
            "Terminate her... immediately!\n",
            "Stand by, Chewie, here we go. Cut in the sublight engines.\n",
            "What the...? Aw, we've come out of hyperspace into a meteor shower. Some kind of asteroid collision. It's not on any of the charts.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   700 of 2,753. Loss: 0.025849102064967155.Elapsed: 0:05:57.\n",
            "0: There you go.\n",
            "Well, wait a minute. Where'd she go? Bring her back! Play back the entire message.\n",
            "What message? The one you're carrying inside your rusty innards!\n",
            "Luke? Luke! Come to dinner!\n",
            "All right, I'll be right there, Aunt Beru.\n",
            "I'm sorry, sir, but he appears to have picked up a slight flutter.\n",
            "Well, see what you can do with him. I'll be right back.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   800 of 2,753. Loss: 0.022461092099547386.Elapsed: 0:06:48.\n",
            "0: The hull is burning up!\n",
            "Time to abandon ship.\n",
            "All the escape pods have been launched.\n",
            "Grievous. Can you fly a cruiser like this?\n",
            "You mean, do I know how to land what's left of this thing?\n",
            "Well?\n",
            "Under the circumstances, I'd say the ability to pilot this thing is irrelevant. Strap yourselves in.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch   900 of 2,753. Loss: 0.021298952400684357.Elapsed: 0:07:39.\n",
            "0: (continuing) Don't worry, this guy's gonna kill himself any minute now!\n",
            "What are you doing? He's gonna blast me!\n",
            "Right - this isn't working.\n",
            "That was too close!\n",
            "Clear that!\n",
            "What??\n",
            "Why didn't the Council have to go after us?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,000 of 2,753. Loss: 0.01695195585489273.Elapsed: 0:08:30.\n",
            "0: That's impossible, even for a computer.\n",
            "It's not impossible. I used to bull's- eye womp rats in my T-sixteen back home. They're not much bigger than two meters.\n",
            "Man your ships! And may the Force be with you!\n",
            "Orbiting the planet at maximum velocity. The moon with the Rebel base will be in range in thirty minutes.\n",
            "This will be a day long remembered. It has seen the end of Kenobi and it will soon see the end of the Rebellion.\n",
            "(over loudspeaker) All flight trooper, man your stations. All flight troops, man your stations.\n",
            "So... you got your reward and you're just leaving then?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,100 of 2,753. Loss: 0.020138956606388092.Elapsed: 0:09:22.\n",
            "0: It's Luke! Chewie!\n",
            "Luke, Luke. Come on!\n",
            "Blast it! Wedge where are you?\n",
            "Thanks, Wedge.\n",
            "(over speaker) Good shooting, Wedge!\n",
            "(over speaker) Red Leader...\n",
            "This is Gold Leader. We're starting out attack run.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,200 of 2,753. Loss: 0.018539827316999435.Elapsed: 0:10:12.\n",
            "0: We are not going to exceed our mandate, my young Padawan learner.\n",
            "I meant in the interest of protecting her, Master, of course.\n",
            "We are not going through this exercise again, Anakin. You will pay attention to my lead.\n",
            "Why?\n",
            "What??!!\n",
            "Why else do you think we were assigned to her, if not to find the killer? Protection is a job for local security... not Jedi. It's overkill, Master. Investigation is implied in our mandate.\n",
            "We will do as the Council has instructed, and you will learn your place, young one.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,300 of 2,753. Loss: 0.023060251027345657.Elapsed: 0:11:04.\n",
            "0: It's not my fault.\n",
            "Sir, we just lost the main rear deflector shield. One more direct hit on the back quarter and we're done for.\n",
            "Turn her around.\n",
            "I said turn her around! I'm going to put all power in the front shield.\n",
            "You're going to attack them?!\n",
            "Sir, the odds of surviving a direct assault on an Imperial Star Destroyer...\n",
            "Shut up!\n",
            "They're moving to attack position! Shields up!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,400 of 2,753. Loss: 0.023187140002846718.Elapsed: 0:11:55.\n",
            "0: Why bother? As a practical matter, the Senate no longer exists.\n",
            "The constitution is in shreds. Amendment after amendment... executive directives, sometimes a dozen in one day.\n",
            "We can't let a thousand years of democracy disappear without a fight.\n",
            "What are you suggesting?\n",
            "I apologize. I didn't mean to sound like a Separatist.\n",
            "We are not Separatists trying to leave the Republic. We are loyalists, trying to preserve democracy in the Republic.\n",
            "It has become increasingly clear to many of us that the Chancellor has become an enemy of democracy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,500 of 2,753. Loss: 0.013721153140068054.Elapsed: 0:12:46.\n",
            "0: Don't make me kill you.\n",
            "Anakin, my allegiance is to the Republic... to democracy.\n",
            "If you're not with me, you're my enemy.\n",
            "Only a Sith Lord deals in absolutes. I will do what I must.\n",
            "You will try.\n",
            "I hear a new apprentice, you have. Emperor, or should I call you Darth Sidious.\n",
            "Master Yoda, you survived.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,600 of 2,753. Loss: 0.015580800361931324.Elapsed: 0:13:37.\n",
            "0: We have a plan which should immobilize the Droid Army. We will send what pilots we have to knock out the Droid control ship which is orbiting the planet. If we can get past their rayshields, we can sever communication and their droids will be helpless.\n",
            "A well-conceived plan. However, there's great risk. The weapons on your fighters may not penetrate the shields on the control ship.\n",
            "And there's an even bigger danger. If the Vicroy escapes, Your Highness, he will return with another droid army.\n",
            "That is why we must not fail to get to the Viceroy. Everything depends on it.\n",
            "beeps\n",
            "she is more foolish than I thought.\n",
            "We are sending all available troops to meet this army of hers assembling near the swamp. It appears to be made up of primitives. We do not expect much resistance.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,700 of 2,753. Loss: 0.018080538138747215.Elapsed: 0:14:29.\n",
            "0: And those control the pitch?\n",
            "You catch on pretty quick.\n",
            "The moment we land the Federation will arrest you, and force you to sign the treaty.\n",
            "I agree I'm not sure what you hope to accomplish by this.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,800 of 2,753. Loss: 0.022563977167010307.Elapsed: 0:15:19.\n",
            "0: It is so wonderful, Annie. You have brought hope to those who have none. I'm so very proud of you\n",
            "Ah, gee enough of this\n",
            "You! You swindled me! You knew the boy was going to win! Somehow you knew it! I lost everything.\n",
            "Whenever you gamble, my friend, eventually you'll lose. Bring the parts to the main hanger. I'll come by your shop later so you can release the boy.\n",
            "You can't have him! It wasn't a fair bet!\n",
            "Would you like to discuss it with the Hutts I'm sure they can settle this.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 1,900 of 2,753. Loss: 0.015994051471352577.Elapsed: 0:16:11.\n",
            "0: They are here! Something's happening... I'm not the Jedi I should be. I am one of the most powerful Jedi, but I'm not satisfied... I want more, and I know I shouldn't.\n",
            "You expect too much of yourself.\n",
            "I have found a way to save you.\n",
            "Save me?\n",
            "From my nightmares.\n",
            "Is that what's bothering you?\n",
            "I won't lose you, Padme.\n",
            "I'm not going to die in childbirth, Annie. I promise you.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,000 of 2,753. Loss: 0.017246760427951813.Elapsed: 0:17:02.\n",
            "0: I'm here, Mom. I'm looking for Shmi Skywalker.\n",
            "Annie?? Little Annie?? Naaaah!!\n",
            "\"(continuing\n",
            "(continuing) You sure sprouted Weehoo! A Jedi! Waddya know? Hey, maybe you couldda help wit some daedbeats who owe...\n",
            "My mother...\n",
            "Oh, yeah. Shmi... she's not mine no more. I sold her.\n",
            "Sold her...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,100 of 2,753. Loss: 0.02063031867146492.Elapsed: 0:17:53.\n",
            "0: I can't do it, Mom. I just can't.\n",
            "Annie\n",
            "Will I ever see you again?\n",
            "What does your heart tell you?\n",
            "I hope so yes I guess.\n",
            "Then we will see each other again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,200 of 2,753. Loss: 0.015308852307498455.Elapsed: 0:18:44.\n",
            "0: Well, we'd better go inside.\n",
            "Master Lars - Master Owen! Somebody to see you!\n",
            "I'm Anakin Skywalker. I'm here looking for my mother.\n",
            "Owen Lars... I guess I'm your step-brother. (they shake hands) This is my girlfriend, Beru.\n",
            "Hello.\n",
            "I'm Padme.\n",
            "I had a feeling you might show up some day.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,300 of 2,753. Loss: 0.021161263808608055.Elapsed: 0:19:35.\n",
            "0: Well, it's only a few guards. This shouldn't be too much trouble.\n",
            "Well, it only takes one to sound the alarm.\n",
            "(with self-confident grin) Then we'll do it real quiet-like.\n",
            "Oh! Oh, my. Uh, Princess Leia!\n",
            "Quiet.\n",
            "I'm afraid our furry companion has gone and done something rather rash.\n",
            "Oh, no.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,400 of 2,753. Loss: 0.015514140017330647.Elapsed: 0:20:26.\n",
            "0: Anakin, there's no time. We must get off the ship before it's too late.\n",
            "He seems to be all right. No broken bones, breathing's all right.\n",
            "Leave him, or we'll never make it.\n",
            "His fate will be the same as ours.\n",
            "Prepare for attack.\n",
            "All batteries fire! Fire!\n",
            "The elevator's not working, (into his comlink) ARTOO...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,500 of 2,753. Loss: 0.01591670885682106.Elapsed: 0:21:17.\n",
            "0: (quickly) Sir -- not in an environment such as this -- that's why I've also been programmed for over thirty secondary functions that...\n",
            "What I really need is a droid that understands the binary language of moisture vaporators.\n",
            "Vaporators! Sir -- My first job was programming binary load lifter... very similar to your vaporators. You could say...\n",
            "Do you speak Bocce?\n",
            "Of course I can, sir. It's like a second language for me... I'm as fluent in Bocce...\n",
            "All right shut up! (turning to Jawa) I'll take this one.\n",
            "Shutting up, sir.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,600 of 2,753. Loss: 0.016239209100604057.Elapsed: 0:22:09.\n",
            "0: (over comlink) Droid of some kind. I didn't hit it that hard. It must have had a self-destruct.\n",
            "(into comlink) An Imperial probe droid.\n",
            "(over comlink) It's a good bet the Empire knows we're here.\n",
            "We'd better start the evacuation.\n",
            "Admiral.\n",
            "Yes, Captain\n",
            "I think we've got something, sir. The report is only a fragment from a probe droid in the Hoth system, but it's the best lead we've had.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Batch 2,700 of 2,753. Loss: 0.018260296434164047.Elapsed: 0:23:00.\n",
            "0: (over comlink) Ahh, good luck.\n",
            "(into comlink) I see you shortly, Rogue Two. We're going to regroup with the others.\n",
            "(into comlink) Just hang on, Dack. We're going to get a lot of firing earlier.\n",
            "(into comlink) Take care, you two. May the Force be with you.\n",
            "Ow!\n",
            "Command station, this is ST 321. Code Clearance Blue. We're starting our approach. Deactivate the security shield.\n",
            "The security deflector shield will be deactivated when we have confirmation of your code transmission. Stand by... You are clear to proceed.\n",
            "\n",
            " Average training loss: 0.02\n",
            " Training epoch took: 0:23:28\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:46:42 (h:mm:ss)\n",
            "Saving model to ./model_save/\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./model_save/tokenizer_config.json',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/vocab.json',\n",
              " './model_save/merges.txt',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class GPT2Dataset(Dataset):\n",
        "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        for txt in txt_list:\n",
        "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "# If the file is not in the same directory, replace the following with the path\n",
        "filename = 'star_wars.txt'\n",
        "with open(filename) as file:\n",
        "    starwars = [line.rstrip() for line in file]\n",
        "\n",
        "nlines = 8\n",
        "min_nlines=3\n",
        "l = len(starwars)\n",
        "starwars_seq = []\n",
        "for i in range(l-nlines):\n",
        "    range_end = min(l-min_nlines,i+nlines)\n",
        "    interaction = '\\n'.join(starwars[i:range_end])\n",
        "    starwars_seq.append(interaction)\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "output_dir = './model_save/'\n",
        "if os.path.exists(output_dir):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "    configuration = GPT2Config.from_pretrained(output_dir)\n",
        "    model = GPT2LMHeadModel.from_pretrained(output_dir, config=configuration)\n",
        "    print(\"Loaded pretrained model.\")\n",
        "else:\n",
        "    # See https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
        "    configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "dataset = GPT2Dataset(starwars_seq, tokenizer, max_length=768)\n",
        "dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "epochs = 2\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "sample_every = 100\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
        "total_steps = len(dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "total_t0 = time.time()\n",
        "training_stats = []\n",
        "model = model.to(device)\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks, token_type_ids=None)\n",
        "        loss = outputs[0]\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "        # Get sample every x batches.\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print(f' Batch {step:>5,} of ' + f'{len(dataloader):>5,}. Loss: {batch_loss:>5,}.' + f'Elapsed: {elapsed}.')\n",
        "            model.eval()\n",
        "            sample_outputs = model.generate(do_sample=True, top_k=50, max_length=768, top_p=0.95, num_return_sequences=1)\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                sample_output_dec = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "                print(f\"{i}: {sample_output_dec}\")\n",
        "            model.train()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(dataloader)\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"\")\n",
        "    print(f\" Average training loss: {avg_train_loss:0.2f}\")\n",
        "    print(f\" Training epoch took: {training_time}\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "timediff = format_time(time.time()-total_t0)\n",
        "print(f\"Total training took {timediff} (h:mm:ss)\")\n",
        "# Saving best-practices: if you use default names for the model, you can reload\n",
        "# it using from_pretrained()\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "print(f\"Saving model to {output_dir}\")\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the model (question 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt > To be or not to be: that is the question.\n",
            "To be or not to be: that is the question. What good is a question of precedure if you start down the south passage and speak of the Rebellion?\n",
            "I know the precedure of the law. I feel confident we can overcome it\n",
            "I must speak with the Jei Council immediately, Your Honor. The situation has become more complicated.\n",
            "Ani, come on.\n",
            "Da queen's a bein grossly nice, mesa tinks. Pitty hot.\n",
            "the Republic is not what it once was. The Senate is full of greedy, squabbling delegates who are only looking out for themselves and their home sytems. There is no interest in the common good no civility, only politics its disgusting. I must be frank, Your Majesty, there is little chance the Senate will act on the invasion.\n",
            "Chancellor Valorum seems to think there is hope.\n",
            "If I may say so, Your Majesty, the Chancellor has little real power he is mired down by baseless accusations of corruption. A manufactured scandal surrounds him\n",
            "=======================================\n"
          ]
        }
      ],
      "source": [
        "text = \"To be or not to be: that is the question.\"\n",
        "print(\"Prompt >\", text)\n",
        "encoded_text = tokenizer(text, return_tensors='pt')\n",
        "encoded_text = encoded_text.to(device)\n",
        "response = model.generate(**encoded_text, max_new_tokens=200, do_sample=True, top_p=0.95, top_k=20)\n",
        "response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "print(response_text)\n",
        "print(\"=======================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50259, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "\n",
        "output_dir = './model_save/'\n",
        "if os.path.exists(output_dir):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "    configuration = GPT2Config.from_pretrained(output_dir)\n",
        "    model = GPT2LMHeadModel.from_pretrained(output_dir, config=configuration)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the model (/w star wars lines - question 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt > the force\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the force is with us, my Master.\n",
            "Welcome home, Lord Tyranus. You have done well.\n",
            "I bring you good news, my Lord. The war has begun.\n",
            "Excellent. (smiling) Everything is going as planned.\n",
            "Where is your apprentice?\n",
            "On his way back to Naboo. He is escorting Senator Amidala home.\n",
            "(continuing) I must admit without the clones, it would not have been a victory.\n",
            "Victory? Victory, you say?\n",
            "=======================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the force grows dark, Anakin, and we are all affected by it. Be wary of your feelings.\n",
            "Anakin, this afternoon the Senate is going to call on me to take direct control of the Jedi Council.\n",
            "The Jedi will no longer report to the Senate?\n",
            "They will report to me... personally. The Senate is too unfocused to conduct a war. This will bring a quick end to things.\n",
            "I agree, but the Jedi Council may not see it that way.\n",
            "There are times when we must all endure adjustments to the constitution in the name of security.\n",
            "With all due respect, sir, the Council is in no mood for more constitutional amendments.\n",
            "Thank you, my friend, but in this case I have no choice... this war must be won.\n",
            "=======================================\n",
            "Prompt > the dark side\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the dark side of the Force is a pathway to many abilities some consider to be unnatural.\n",
            "What happened to him?\n",
            "He became so powerful... the only thing he was afraid of was losing his power, which eventually, of course, he did. Unfortunately, he taught his apprentice everything he knew, then his apprentice killed him in his sleep. (smiles) Plagueis never saw it coming. It's ironic he could save others from death, but not himself.\n",
            "Is it possible to learn this power?\n",
            "Not from a Jedi.\n",
            "(holo) Palpatine thinks General Grievous is on Utapau. We have had no reports of this from our agents.\n",
            "(holo) How could the Chancellor have come by this information and we know nothing about it? We have had contact with Baron Papanoida and he said no one was there.\n",
            "=======================================\n",
            "the dark side of the Force is a pathway to many abilities some consider to be unnatural.\n",
            "What happened to him?\n",
            "He became so powerful... the only thing he was afraid of was losing his power, which eventually, of course, he did. Unfortunately, he taught his apprentice everything he knew, then his apprentice killed him in his sleep. (smiles) Plagueis never saw it coming. It's ironic he could save others from death, but not himself.\n",
            "Is it possible to learn this power?\n",
            "Not from a Jedi.\n",
            "(holo) Palpatine thinks General Grievous is on Utapau. We have had no reports of this from our agents.\n",
            "(holo) How could the Chancellor have come by this information and we know nothing about it? We have had contact with Baron Papanoida and he said no one was there.\n",
            "=======================================\n"
          ]
        }
      ],
      "source": [
        "sw_prompts = ['the force', 'the dark side']\n",
        "for text in sw_prompts:\n",
        "    print(\"Prompt >\", text)\n",
        "    encoded_text = tokenizer(text, return_tensors='pt')\n",
        "    encoded_text = encoded_text.to(device)\n",
        "    for i in range(2):\n",
        "        response = model.generate(**encoded_text, max_new_tokens=200, do_sample=True, top_k=20, top_p=0.95)\n",
        "        response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "        print(response_text)\n",
        "        print(\"=======================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Counting model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n",
            "The GPT-2 model has 148 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "transformer.wte.weight                                  (50259, 768)\n",
            "transformer.wpe.weight                                   (1024, 768)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "transformer.h.0.ln_1.weight                                   (768,)\n",
            "transformer.h.0.ln_1.bias                                     (768,)\n",
            "transformer.h.0.attn.c_attn.weight                       (768, 2304)\n",
            "transformer.h.0.attn.c_attn.bias                             (2304,)\n",
            "transformer.h.0.attn.c_proj.weight                        (768, 768)\n",
            "transformer.h.0.attn.c_proj.bias                              (768,)\n",
            "transformer.h.0.ln_2.weight                                   (768,)\n",
            "transformer.h.0.ln_2.bias                                     (768,)\n",
            "transformer.h.0.mlp.c_fc.weight                          (768, 3072)\n",
            "transformer.h.0.mlp.c_fc.bias                                (3072,)\n",
            "transformer.h.0.mlp.c_proj.weight                        (3072, 768)\n",
            "transformer.h.0.mlp.c_proj.bias                               (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "transformer.ln_f.weight                                       (768,)\n",
            "transformer.ln_f.bias                                         (768,)\n"
          ]
        }
      ],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:2]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "for p in params[2:14]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "for p in params[-2:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bonus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 8.688618659973145\n",
            "Epoch: 2, Loss: 8.552766799926758\n",
            "Epoch: 3, Loss: 8.483601570129395\n",
            "Epoch: 4, Loss: 8.427410125732422\n",
            "Epoch: 5, Loss: 8.37089729309082\n",
            "Validation Loss: 8.656967163085938\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        \n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads # Number of attention heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "        \n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "        \n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        \n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        \n",
        "        # Multiply by values to obtain the final output\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "        \n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "        \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "        \n",
        "        # Perform scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        \n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            # Does not apply mask\n",
        "            enc_output = enc_layer(enc_output, None)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            # Does not apply encoder mask\n",
        "            dec_output = dec_layer(dec_output, enc_output, None, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output\n",
        "\n",
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(5):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    # print(output)\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "transformer.eval()\n",
        "\n",
        "# Generate random sample validation data\n",
        "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
        "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
        "    print(f\"Validation Loss: {val_loss.item()}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FtnO_3cDW4Vp"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
